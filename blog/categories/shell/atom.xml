<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: shell | linfan's blog]]></title>
  <link href="http://linfan.info/blog/categories/shell/atom.xml" rel="self"/>
  <link href="http://linfan.info/"/>
  <updated>2016-10-07T10:03:55+08:00</updated>
  <id>http://linfan.info/</id>
  <author>
    <name><![CDATA[linfan]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[用Shell实现简单的Web爬虫]]></title>
    <link href="http://linfan.info/blog/2013/09/15/web-crawler-by-shell/"/>
    <updated>2013-09-15T22:47:00+08:00</updated>
    <id>http://linfan.info/blog/2013/09/15/web-crawler-by-shell</id>
    <content type="html"><![CDATA[<p>对于结构不太复杂的网页，使用grep和sed分析提取目标URL，之后使用wget下载。</p>

<p>以下是两个例子。</p>

<!--more-->


<ul>
<li>抓取<a href="http://jandan.net">煎蛋</a>“妹子图”栏目</li>
</ul>


<p><div><script src='https://gist.github.com/6571304.js'></script>
<noscript><pre><code>#!/bin/sh

delay=0
timeout=10
retry=2

usage()
{
	echo &quot;Usage: `basename $0` frompage topage&quot;
	exit 1
}

[ $# -ne 2 ] &amp;&amp; usage
[ $1 -le 0 ] || [ $2 -le 0 ] || [ $1 -gt $2 ] &amp;&amp; usage

page=$1
while [ $page -le $2 ] ; do
	echo &quot;[INFO] Downloading page $page ...&quot;

	imgs=`wget -q -O - http://jandan.net/ooxx/page-$page | grep -ioE '&lt;img src=&quot;http.*\.(jpg|gif)&quot; */&gt;' | sed 's/&lt;[Ii][Mm][Gg].*[Ss][Rr][Cc]\s*=\s*&quot;*//;s/[&quot; ].*//'`
	[ -z &quot;$imgs&quot; ] &amp;&amp; {
		echo &quot;[WARNING] Failed to get image list of page $page&quot;
		page=$((page+1))
		continue
	}	   

	index=1
	for img in $imgs ; do
		wget -q --tries=$retry --timeout=$timeout -O &quot;$page-$index.${img##*.}&quot; &quot;$img&quot;
		if [ $? -ne 0 ] ; then
			rm -f &quot;$page-$index.${img##*.}&quot;
			echo &quot;[WARNING] Failed to download $img&quot;
		else
			index=$((index+1))
		fi
	done

	page=$((page+1))
	[ $page -le $2 ] &amp;&amp; sleep $delay
done

exit 0
</code></pre></noscript></div>
</p>

<ul>
<li>抓取<a href="http://www.panoramio.com">Panoramio</a>网站某个用户上传的所有照片</li>
</ul>


<p><div><script src='https://gist.github.com/6606109.js'></script>
<noscript><pre><code>#!/bin/sh

delay=0
timeout=10
retry=2

usage()
{
	echo &quot;Usage: `basename $0` userid&quot;
	exit 1
}

[ $# -ne 1 ] &amp;&amp; usage

global_index=1
page=1
while true ; do
	echo &quot;[INFO] Processing page $page&quot;
	wget -q -O .tmp http://www.panoramio.com/user/$1?photo_page=$page
	if [ $? -ne 0 ] ; then
		echo &quot;[WARNING] Can not get content of page $page&quot;
		break
	fi

	if [ -z &quot;$total&quot; ] ; then
		total=`grep -iE '&lt;a href=&quot;\/user\/.*\/stats' .tmp | head -n 1 | sed 's/.*stats&quot;&gt;//;s/&lt;.*//'`
	fi
	imgs=`grep -ioE '&lt;a href=&quot;\/photo\/[0-9]*&quot;$' .tmp | sed 's/.*\///;s/&quot;//'`
	alts=`grep -ioE '^              &gt;..*&lt;\/a&gt;$' .tmp | sed 's/&lt;.*//;s/.*&gt;//'`

	index=1
	for img in $imgs ; do
		url=http://static.panoramio.com/photos/original/$img.jpg
		alt=`{ for alt in $alts ; do echo $alt ; done } | head -n $index | tail -n 1`
		file=`echo $global_index | awk '{printf &quot;%03d&quot;, $0}'`_$alt.jpg
		index=$((index+1))
		global_index=$((global_index+1))

		if [ -f $file ] ; then
			continue
		fi

		echo &quot;[INFO] Downloading $url&quot;
		wget -q --tries=$retry --timeout=$timeout -O &quot;$file&quot; &quot;$url&quot;
		if [ $? -ne 0 ] ; then
			rm -f &quot;$file&quot;
			echo &quot;[WARNING] Failed to download $url to $file&quot;
		fi
		sleep $delay
	done

	if [ $global_index -gt $total ] ; then
		break
	fi	

	page=$((page+1))
done

rm -f .tmp

exit 0
</code></pre></noscript></div>
</p>
]]></content>
  </entry>
  
</feed>
